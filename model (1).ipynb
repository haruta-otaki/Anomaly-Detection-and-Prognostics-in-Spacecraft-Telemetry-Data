{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2cfce4d-82c7-43cf-a818-594b73f5d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "from pathlib import Path\n",
    "import os\n",
    "import ast\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, fbeta_score, f1_score\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sktime.transformations.panel.rocket import (\n",
    "    MiniRocket,\n",
    "    MiniRocketMultivariate,\n",
    "    MiniRocketMultivariateVariable,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecfa9908-f807-42be-b37c-be84a9b87ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection of dataset information for each telemetry data \n",
    "training_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "training_df = []\n",
    "test_df = []\n",
    "\n",
    "file_names = []\n",
    "\n",
    "label_df = pd.read_csv(\"labeled_anomalies.csv\")\n",
    "\n",
    "train_data_path = \"npy_train\" \n",
    "test_data_path = \"npy_test\" \n",
    "\n",
    "os.makedirs(\"raw_train\", exist_ok=True)\n",
    "os.makedirs(\"raw_test\", exist_ok=True)\n",
    "\n",
    "index = 0\n",
    "for root, _, files in os.walk(train_data_path):\n",
    "    for file in files:\n",
    "        example_path = os.path.join(root, file)\n",
    "        filename = Path(example_path).stem\n",
    "        \n",
    "        row_indices = label_df[label_df['chan_id'] == filename].index.tolist()\n",
    "        if not row_indices:\n",
    "            continue\n",
    "        spacecraft = label_df.loc[row_indices[0], 'spacecraft']\n",
    "        \n",
    "        if (spacecraft == \"SMAP\"): \n",
    "            data = np.load(example_path)\n",
    "            df = pd.DataFrame(data)\n",
    "            print(\"(train) df: \", filename, df.shape, \"index: \", index)\n",
    "            df.to_csv(f\"raw_train/{filename}.csv\", index=False)\n",
    "            training_dfs.append(df) \n",
    "            index += 1\n",
    "\n",
    "index = 0\n",
    "for root, _, files in os.walk(test_data_path):\n",
    "    for file in files:\n",
    "        example_path = os.path.join(root, file)\n",
    "        filename = Path(example_path).stem\n",
    "        \n",
    "        row_indices = label_df[label_df['chan_id'] == filename].index.tolist()\n",
    "        if not row_indices:\n",
    "            continue\n",
    "        spacecraft = label_df.loc[row_indices[0], 'spacecraft']\n",
    "        \n",
    "        if (spacecraft == \"SMAP\"): \n",
    "            data = np.load(example_path)\n",
    "            df = pd.DataFrame(data)\n",
    "            print(\"(test) df: \", filename, df.shape, \"index: \", index)\n",
    "            test_dfs.append(df) \n",
    "            file_names.append(filename)\n",
    "            \n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d49f058b-3a33-4b1a-a147-81c717e1645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    training_dfs = {}\n",
    "    test_dfs = {}\n",
    "    file_names = {\"train\": {}, \"test\": {}}\n",
    "    label_df = pd.read_csv(\"labeled_anomalies.csv\")\n",
    "    train_data_path = \"raw_train\" \n",
    "    test_data_path = \"raw_test\" \n",
    "    \n",
    "    for root, _, files in os.walk(train_data_path):\n",
    "        for file in files:\n",
    "            example_path = os.path.join(root, file)\n",
    "            filename = Path(example_path).stem\n",
    "            df = pd.read_csv(example_path)\n",
    "            channel = filename[0]            \n",
    "            if (training_dfs.get(channel) == None):\n",
    "                training_dfs[channel] = []\n",
    "                file_names[\"train\"][channel] = []\n",
    "                training_dfs[channel].append(df)\n",
    "                file_names[\"train\"][channel].append(filename)\n",
    "            else:\n",
    "                training_dfs[channel].append(df)\n",
    "                file_names[\"train\"][channel].append(filename)\n",
    "    \n",
    "    for root, _, files in os.walk(test_data_path):\n",
    "        for file in files:\n",
    "            example_path = os.path.join(root, file)\n",
    "            filename = Path(example_path).stem\n",
    "            df = pd.read_csv(example_path)\n",
    "            channel = filename[0]\n",
    "            if (test_dfs.get(channel) == None):\n",
    "                test_dfs[channel] = []\n",
    "                file_names[\"test\"][channel] = []\n",
    "                test_dfs[channel].append(df)\n",
    "                file_names[\"test\"][channel].append(filename)\n",
    "            else:\n",
    "                test_dfs[channel].append(df)\n",
    "                file_names[\"test\"][channel].append(filename)\n",
    "\n",
    "    return (training_dfs, test_dfs, label_df, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7af7343-fd52-45db-9b3a-0cf2358da275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(training_dfs, test_dfs, window_size, window_overlap, label_df, file_names):\n",
    "    training_data = {}\n",
    "    test_data = {}\n",
    "    label_data = {}\n",
    "    \n",
    "    for channel in training_dfs.keys():\n",
    "        training_data[channel] = []\n",
    "        for df in training_dfs[channel]: \n",
    "            for i in range(0, len(df), window_overlap):\n",
    "                window = []\n",
    "                if i + window_size > len(df): \n",
    "                    window = df.iloc[-window_size:].to_numpy().tolist()\n",
    "                else: \n",
    "                    window = df.iloc[i:i + window_size].to_numpy().tolist()\n",
    "                np_window = np.array(window)\n",
    "                transposed_window = np_window.T\n",
    "                normal_window = transposed_window.tolist() \n",
    "                training_data[channel].append(normal_window)\n",
    "                \n",
    "    for channel in test_dfs.keys():\n",
    "        test_data[channel] = []\n",
    "        label_data[channel] = []\n",
    "        for i in range(len(test_dfs[channel])):\n",
    "            df = test_dfs[channel][i]\n",
    "            for j in range(0, len(df), window_overlap):\n",
    "                window = []\n",
    "                if j + window_size > len(df): \n",
    "                    window = df.iloc[-window_size:].to_numpy().tolist()\n",
    "                else: \n",
    "                    window = df.iloc[j:j + window_size].to_numpy().tolist()\n",
    "                row_indices = label_df[label_df[\"chan_id\"] == file_names[\"test\"][channel][i]].index.tolist()\n",
    "                \n",
    "                if not row_indices:\n",
    "                    continue\n",
    "\n",
    "                anomaly_sequence = label_df.loc[row_indices[0], 'anomaly_sequences']\n",
    "                anomaly_sequence = ast.literal_eval(anomaly_sequence)\n",
    "                labeled = False\n",
    "                for anomalies in anomaly_sequence:\n",
    "                    if (not(anomalies[1] <= j or anomalies[0] >= j + window_size)) and labeled == False:\n",
    "                        label_data[channel].append(1)\n",
    "                        labeled = True\n",
    "                if labeled == False:\n",
    "                    label_data[channel].append(0)\n",
    "                np_window = np.array(window)\n",
    "                transposed_window = np_window.T\n",
    "                normal_window = transposed_window.tolist() \n",
    "                test_data[channel].append(normal_window)\n",
    "                \n",
    "    return (training_data, test_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d0c50c2-a610-488e-8e78-a346d393de6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  A shape:  (842, 25, 60)\n",
      "train:  P shape:  (703, 25, 60)\n",
      "train:  F shape:  (432, 25, 60)\n",
      "train:  D shape:  (1445, 25, 60)\n",
      "train:  R shape:  (144, 25, 60)\n",
      "train:  G shape:  (793, 25, 60)\n",
      "train:  T shape:  (431, 25, 60)\n",
      "train:  E shape:  (1867, 25, 60)\n",
      "train:  S shape:  (141, 25, 60)\n",
      "train:  B shape:  (122, 25, 60)\n",
      "test:  E shape:  (5486, 25, 60)\n",
      "test:  T shape:  (1292, 25, 60)\n",
      "test:  G shape:  (2405, 25, 60)\n",
      "test:  D shape:  (4789, 25, 60)\n",
      "test:  A shape:  (3374, 25, 60)\n",
      "test:  F shape:  (1281, 25, 60)\n",
      "test:  P shape:  (2056, 25, 60)\n",
      "test:  B shape:  (403, 25, 60)\n",
      "test:  R shape:  (363, 25, 60)\n",
      "test:  S shape:  (367, 25, 60)\n",
      "label:  E shape:  (5486,)\n",
      "label:  T shape:  (1292,)\n",
      "label:  G shape:  (2405,)\n",
      "label:  D shape:  (4789,)\n",
      "label:  A shape:  (3374,)\n",
      "label:  F shape:  (1281,)\n",
      "label:  P shape:  (2056,)\n",
      "label:  B shape:  (403,)\n",
      "label:  R shape:  (363,)\n",
      "label:  S shape:  (367,)\n"
     ]
    }
   ],
   "source": [
    "window_size = 60 \n",
    "window_overlap = 20\n",
    "training_dfs, test_dfs, label_df, file_names = get_data()\n",
    "\n",
    "X_train_collection, X_test_collection, y_test_collection = create_windows(training_dfs, test_dfs, window_size, window_overlap, label_df, file_names)\n",
    "\n",
    "data = {}\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "for channel in X_train_collection:\n",
    "    if data.get(channel) is None:\n",
    "        data[channel] = {}\n",
    "    if data.get(channel).get(\"train\") is None:\n",
    "        X_train = np.array(X_train_collection[channel])\n",
    "        path = f\"data/{channel}/train\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        np.save(path + f\"/{channel}.npy\", X_train)\n",
    "        data[channel][\"train\"] = path + f\"/{channel}.npy\"\n",
    "        print(\"train: \", channel, \"shape: \", X_train.shape)\n",
    "    else:\n",
    "        print(\"Error: duplicate training data sets\")\n",
    "   \n",
    "    \n",
    "for channel in X_test_collection: \n",
    "    if data.get(channel) is None:\n",
    "        continue\n",
    "    if data.get(channel).get(\"test\") is None:\n",
    "        X_test = np.array(X_test_collection[channel])\n",
    "        path = f\"data/{channel}/test\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        np.save(path + f\"/{channel}.npy\", X_test)\n",
    "        data[channel][\"test\"] = path + f\"/{channel}.npy\"\n",
    "        print(\"test: \", channel, \"shape: \", X_test.shape)\n",
    "    else:\n",
    "        print(\"Error: duplicate testing data sets\")\n",
    "   \n",
    "  \n",
    "for channel in y_test_collection:\n",
    "    if data.get(channel) is None:\n",
    "        continue\n",
    "    if data.get(channel).get(\"label\") is None:\n",
    "        y_test = np.array(y_test_collection[channel])\n",
    "        path = f\"data/{channel}/label\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        np.save(path + f\"/{channel}.npy\", y_test)\n",
    "        data[channel][\"label\"] = path + f\"/{channel}.npy\"\n",
    "        print(\"label: \", channel, \"shape: \", y_test.shape)\n",
    "    else:\n",
    "        print(\"Error: duplicate label data sets\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62abdbd-26c4-4901-b050-32642e436af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'G', 'S', 'P', 'D', 'F', 'R', 'A', 'E', 'B']\n"
     ]
    }
   ],
   "source": [
    "channel_names = []\n",
    "\n",
    "for channel in os.listdir(\"data\"):\n",
    "    if os.path.isdir(os.path.join(\"data\", channel)):\n",
    "        channel_names.append(channel)\n",
    "print(channel_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c17dc62-a4ae-410f-a25e-1cf83cccd79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  T shape:  (431, 25, 60)\n",
      "test:  T shape:  (1292, 25, 60)\n",
      "label:  T shape:  (1292,)\n",
      "train:  G shape:  (793, 25, 60)\n",
      "test:  G shape:  (2405, 25, 60)\n",
      "label:  G shape:  (2405,)\n",
      "train:  S shape:  (141, 25, 60)\n",
      "test:  S shape:  (367, 25, 60)\n",
      "label:  S shape:  (367,)\n",
      "train:  P shape:  (703, 25, 60)\n",
      "test:  P shape:  (2056, 25, 60)\n",
      "label:  P shape:  (2056,)\n",
      "train:  D shape:  (1445, 25, 60)\n",
      "test:  D shape:  (4789, 25, 60)\n",
      "label:  D shape:  (4789,)\n",
      "train:  F shape:  (432, 25, 60)\n",
      "test:  F shape:  (1281, 25, 60)\n",
      "label:  F shape:  (1281,)\n",
      "train:  R shape:  (144, 25, 60)\n",
      "test:  R shape:  (363, 25, 60)\n",
      "label:  R shape:  (363,)\n",
      "train:  A shape:  (842, 25, 60)\n",
      "test:  A shape:  (3374, 25, 60)\n",
      "label:  A shape:  (3374,)\n",
      "train:  E shape:  (1867, 25, 60)\n",
      "test:  E shape:  (5486, 25, 60)\n",
      "label:  E shape:  (5486,)\n",
      "train:  B shape:  (122, 25, 60)\n",
      "test:  B shape:  (403, 25, 60)\n",
      "label:  B shape:  (403,)\n"
     ]
    }
   ],
   "source": [
    "raw_data= {}\n",
    "for channel in channel_names:\n",
    "    raw_data[channel] = {}\n",
    "    training_data_path = f\"data/{channel}/train/{channel}.npy\"\n",
    "    testing_data_path = f\"data/{channel}/test/{channel}.npy\"\n",
    "    label_data_path = f\"data/{channel}/label/{channel}.npy\"\n",
    "    \n",
    "    X_train = np.load(training_data_path)\n",
    "    X_test = np.load(testing_data_path)\n",
    "    y_test = np.load(label_data_path)\n",
    "\n",
    "    raw_data[channel][\"train\"] = X_train\n",
    "    print(\"train: \", channel, \"shape: \", X_train.shape)\n",
    "    raw_data[channel][\"test\"] = X_test\n",
    "    print(\"test: \", channel, \"shape: \", X_test.shape)\n",
    "    raw_data[channel][\"label\"] = y_test\n",
    "    print(\"label: \", channel, \"shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26df1794-f418-4731-8425-ee5aae3be8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DAVIDSON/haotaki/.local/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:373: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  T shape:  (431, 9996)\n",
      "test:  T shape:  (1292, 9996)\n",
      "label:  T shape:  (1292,)\n",
      "train:  G shape:  (793, 9996)\n",
      "test:  G shape:  (2405, 9996)\n",
      "label:  G shape:  (2405,)\n",
      "train:  S shape:  (141, 9996)\n",
      "test:  S shape:  (367, 9996)\n",
      "label:  S shape:  (367,)\n",
      "train:  P shape:  (703, 9996)\n",
      "test:  P shape:  (2056, 9996)\n",
      "label:  P shape:  (2056,)\n",
      "train:  D shape:  (1445, 9996)\n",
      "test:  D shape:  (4789, 9996)\n",
      "label:  D shape:  (4789,)\n",
      "train:  F shape:  (432, 9996)\n",
      "test:  F shape:  (1281, 9996)\n",
      "label:  F shape:  (1281,)\n",
      "train:  R shape:  (144, 9996)\n",
      "test:  R shape:  (363, 9996)\n",
      "label:  R shape:  (363,)\n",
      "train:  A shape:  (842, 9996)\n",
      "test:  A shape:  (3374, 9996)\n",
      "label:  A shape:  (3374,)\n",
      "train:  E shape:  (1867, 9996)\n",
      "test:  E shape:  (5486, 9996)\n",
      "label:  E shape:  (5486,)\n",
      "train:  B shape:  (122, 9996)\n",
      "test:  B shape:  (403, 9996)\n",
      "label:  B shape:  (403,)\n",
      "stoe:  T\n",
      "stoe:  G\n",
      "stoe:  S\n",
      "stoe:  P\n",
      "stoe:  D\n",
      "stoe:  F\n",
      "stoe:  R\n",
      "stoe:  A\n",
      "stoe:  E\n",
      "stoe:  B\n"
     ]
    }
   ],
   "source": [
    "transformed_data= {}\n",
    "os.makedirs(\"transformed_data\", exist_ok=True)\n",
    "\n",
    "for channel in channel_names:\n",
    "    transformed_data[channel] = {}\n",
    "    training_data_path = f\"data/{channel}/train/{channel}.npy\"\n",
    "    testing_data_path = f\"data/{channel}/test/{channel}.npy\"\n",
    "    label_data_path = f\"data/{channel}/label/{channel}.npy\"\n",
    "    \n",
    "    X_train = np.load(training_data_path)\n",
    "    X_test = np.load(testing_data_path)\n",
    "    y_test = np.load(label_data_path)\n",
    "\n",
    "    minirocket = MiniRocketMultivariate(num_kernels=10000, n_jobs = 2, random_state = 42) \n",
    "    minirocket.fit(X_train)\n",
    "    X_transform_train = minirocket.transform(X_train)\n",
    "    X_transform_test = minirocket.transform(X_test)\n",
    "\n",
    "    transformed_data[channel][\"train\"] = X_transform_train\n",
    "    print(\"train: \", channel, \"shape: \", X_transform_train.shape)\n",
    "    transformed_data[channel][\"test\"] = X_transform_test\n",
    "    print(\"test: \", channel, \"shape: \", X_transform_test.shape)\n",
    "    transformed_data[channel][\"label\"] = y_test\n",
    "    print(\"label: \", channel, \"shape: \", y_test.shape)\n",
    "\n",
    "for channel in transformed_data: \n",
    "    print(\"store: \", channel)\n",
    "    X_transform_train = transformed_data[channel][\"train\"]\n",
    "    X_transform_test = transformed_data[channel][\"test\"]\n",
    "    y_test = transformed_data[channel][\"label\"]\n",
    "\n",
    "    train_df = pd.DataFrame(X_transform_train)\n",
    "    test_df = pd.DataFrame(X_transform_test)\n",
    "    label_df = pd.DataFrame(y_test)\n",
    "\n",
    "    training_data_path = f\"transformed_data/{channel}/train\"\n",
    "    testing_data_path = f\"transformed_data/{channel}/test\"\n",
    "    label_data_path = f\"transformed_data/{channel}/label\"\n",
    "    \n",
    "    os.makedirs(training_data_path, exist_ok=True)\n",
    "    os.makedirs(testing_data_path, exist_ok=True)\n",
    "    os.makedirs(label_data_path, exist_ok=True)\n",
    "\n",
    "    train_df.to_csv(training_data_path + f\"/{channel}.csv\", index=False)\n",
    "    test_df.to_csv(testing_data_path + f\"/{channel}.csv\", index=False)\n",
    "    label_df.to_csv(label_data_path + f\"/{channel}.csv\", index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce77693b-1d71-449e-b769-a9cf8ca798d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  T shape:  (431, 9996)\n",
      "test:  T shape:  (1292, 9996)\n",
      "label:  T shape:  (1292, 1)\n",
      "train:  G shape:  (793, 9996)\n",
      "test:  G shape:  (2405, 9996)\n",
      "label:  G shape:  (2405, 1)\n",
      "train:  S shape:  (141, 9996)\n",
      "test:  S shape:  (367, 9996)\n",
      "label:  S shape:  (367, 1)\n",
      "train:  P shape:  (703, 9996)\n",
      "test:  P shape:  (2056, 9996)\n",
      "label:  P shape:  (2056, 1)\n",
      "train:  D shape:  (1445, 9996)\n",
      "test:  D shape:  (4789, 9996)\n",
      "label:  D shape:  (4789, 1)\n",
      "train:  F shape:  (432, 9996)\n",
      "test:  F shape:  (1281, 9996)\n",
      "label:  F shape:  (1281, 1)\n",
      "train:  R shape:  (144, 9996)\n",
      "test:  R shape:  (363, 9996)\n",
      "label:  R shape:  (363, 1)\n",
      "train:  A shape:  (842, 9996)\n",
      "test:  A shape:  (3374, 9996)\n",
      "label:  A shape:  (3374, 1)\n",
      "train:  E shape:  (1867, 9996)\n",
      "test:  E shape:  (5486, 9996)\n",
      "label:  E shape:  (5486, 1)\n",
      "train:  B shape:  (122, 9996)\n",
      "test:  B shape:  (403, 9996)\n",
      "label:  B shape:  (403, 1)\n"
     ]
    }
   ],
   "source": [
    "min_max_fitted_data = {}\n",
    "\n",
    "for channel in channel_names: \n",
    "    min_max_fitted_data[channel] = {}\n",
    "\n",
    "    training_data_path = f\"transformed_data/{channel}/train/{channel}.csv\"\n",
    "    testing_data_path = f\"transformed_data/{channel}/test/{channel}.csv\"\n",
    "    label_data_path = f\"transformed_data/{channel}/label/{channel}.csv\"\n",
    "    \n",
    "    X_transform_train = pd.read_csv(training_data_path).to_numpy()\n",
    "    X_transform_test =  pd.read_csv(testing_data_path).to_numpy()\n",
    "    y_test = pd.read_csv(label_data_path).to_numpy()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_transform_train)\n",
    "    X_fit_train = scaler.transform(X_transform_train)\n",
    "    X_fit_test = scaler.transform(X_transform_test)\n",
    "    \n",
    "    min_max_fitted_data[channel][\"train\"] = X_fit_train\n",
    "    min_max_fitted_data[channel][\"test\"] = X_fit_test\n",
    "    min_max_fitted_data[channel][\"label\"] = y_test\n",
    "    \n",
    "    print(\"train: \", channel, \"shape: \", X_fit_train.shape)\n",
    "    print(\"test: \", channel, \"shape: \", X_fit_test.shape)\n",
    "    print(\"label: \", channel, \"shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf043f3-518d-4026-96b4-b50e79cfa290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  T shape:  (431, 9996)\n",
      "test:  T shape:  (1292, 9996)\n",
      "label:  T shape:  (1292, 1)\n",
      "train:  G shape:  (793, 9996)\n",
      "test:  G shape:  (2405, 9996)\n",
      "label:  G shape:  (2405, 1)\n",
      "train:  S shape:  (141, 9996)\n",
      "test:  S shape:  (367, 9996)\n",
      "label:  S shape:  (367, 1)\n",
      "train:  P shape:  (703, 9996)\n",
      "test:  P shape:  (2056, 9996)\n",
      "label:  P shape:  (2056, 1)\n",
      "train:  D shape:  (1445, 9996)\n",
      "test:  D shape:  (4789, 9996)\n",
      "label:  D shape:  (4789, 1)\n",
      "train:  F shape:  (432, 9996)\n",
      "test:  F shape:  (1281, 9996)\n",
      "label:  F shape:  (1281, 1)\n",
      "train:  R shape:  (144, 9996)\n",
      "test:  R shape:  (363, 9996)\n",
      "label:  R shape:  (363, 1)\n",
      "train:  A shape:  (842, 9996)\n",
      "test:  A shape:  (3374, 9996)\n",
      "label:  A shape:  (3374, 1)\n",
      "train:  E shape:  (1867, 9996)\n",
      "test:  E shape:  (5486, 9996)\n",
      "label:  E shape:  (5486, 1)\n",
      "train:  B shape:  (122, 9996)\n",
      "test:  B shape:  (403, 9996)\n",
      "label:  B shape:  (403, 1)\n"
     ]
    }
   ],
   "source": [
    "standard_fitted_data = {}\n",
    "\n",
    "for channel in channel_names: \n",
    "    standard_fitted_data[channel] = {}\n",
    "\n",
    "    training_data_path = f\"transformed_data/{channel}/train/{channel}.csv\"\n",
    "    testing_data_path = f\"transformed_data/{channel}/test/{channel}.csv\"\n",
    "    label_data_path = f\"transformed_data/{channel}/label/{channel}.csv\"\n",
    "    \n",
    "    X_transform_train = pd.read_csv(training_data_path).to_numpy()\n",
    "    X_transform_test =  pd.read_csv(testing_data_path).to_numpy()\n",
    "    y_test = pd.read_csv(label_data_path).to_numpy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_transform_train)\n",
    "    X_fit_train = scaler.transform(X_transform_train)\n",
    "    X_fit_test = scaler.transform(X_transform_test)\n",
    "    \n",
    "    standard_fitted_data[channel][\"train\"] = X_fit_train\n",
    "    standard_fitted_data[channel][\"test\"] = X_fit_test\n",
    "    standard_fitted_data[channel][\"label\"] = y_test\n",
    "    \n",
    "    print(\"train: \", channel, \"shape: \", X_fit_train.shape)\n",
    "    print(\"test: \", channel, \"shape: \", X_fit_test.shape)\n",
    "    print(\"label: \", channel, \"shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d722ab9f-a61f-4f36-8347-a54434153f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel:  T\n",
      "anomaly_rate:  0.14628482972136223\n",
      "channel:  G\n",
      "anomaly_rate:  0.021205821205821207\n",
      "channel:  S\n",
      "anomaly_rate:  0.0681198910081744\n",
      "channel:  P\n",
      "anomaly_rate:  0.14348249027237353\n",
      "channel:  D\n",
      "anomaly_rate:  0.22635205679682607\n",
      "channel:  F\n",
      "anomaly_rate:  0.12724434035909446\n",
      "channel:  R\n",
      "anomaly_rate:  0.01928374655647383\n",
      "channel:  A\n",
      "anomaly_rate:  0.16478956727919383\n",
      "channel:  E\n",
      "anomaly_rate:  0.11009843237331389\n",
      "channel:  B\n",
      "anomaly_rate:  0.01488833746898263\n",
      "[0.14628482972136223, 0.021205821205821207, 0.0681198910081744, 0.14348249027237353, 0.22635205679682607, 0.12724434035909446, 0.01928374655647383, 0.16478956727919383, 0.11009843237331389, 0.01488833746898263]\n"
     ]
    }
   ],
   "source": [
    "# performance bad for channels B, R, and S due to their small training data size\n",
    "# nu - controls boundary tightness (Smaller nu → tighter boundary → fewer false positives, Larger nu → looser boundary → higher recall)\n",
    "# threshold\t- converts scores (anomaly / normal)\n",
    "\n",
    "# Large (1000+ windows/channel)\tMostly clean\t0.01 – 0.02\n",
    "# Medium (hundreds of windows)\tMostly clean\t0.02 – 0.05\n",
    "# Small or noisy\tSome anomalies in training\t0.05 – 0.1\n",
    "#tighter nu makes more sense as the training data is supposed to have only normal data - needs confirmation at this point in time \n",
    "\n",
    "anomaly_rate = []\n",
    "for channel in standard_fitted_data: \n",
    "    print(\"channel: \", channel)\n",
    "    anomaly_count = 0\n",
    "    for label in standard_fitted_data[channel][\"label\"]:\n",
    "        if label == 1:\n",
    "            anomaly_count += 1\n",
    "    anomaly_rate.append(anomaly_count / len(standard_fitted_data[channel][\"label\"]))\n",
    "    print(\"anomaly_rate: \", anomaly_count / len(standard_fitted_data[channel][\"label\"]))\n",
    "\n",
    "print(anomaly_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "183b7ba3-ab8b-4f27-af59-6786a12b5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(threshold, decision_scores):\n",
    "    y_pred = []\n",
    "    for score in decision_scores: \n",
    "        if score < threshold: \n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c299bcd5-4795-4e06-a8de-cf0c38e70a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_class_svm(channel_data, channel):\n",
    "    print(\"channel:\", channel)\n",
    "    clf = OneClassSVM().fit(channel_data[\"train\"])\n",
    "    decision_scores = clf.decision_function(channel_data[\"test\"])\n",
    "    \n",
    "    percentiles = np.arange(1, 30 + 1)\n",
    "    thresholds = []\n",
    "    for percent in percentiles: \n",
    "        thresholds.append(np.percentile(decision_scores, percent))\n",
    "    \n",
    "    # best_f1_score = 0\n",
    "    best_scores = (0, 0, 0)  \n",
    "\n",
    "    for th in thresholds:\n",
    "        y_pred = predict(th, decision_scores)\n",
    "        # f1 = f1_score(channel_data[\"label\"], y_pred)\n",
    "        recall = recall_score(channel_data[\"label\"], y_pred, pos_label=1)\n",
    "        precision = precision_score(channel_data[\"label\"], y_pred, zero_division=0)\n",
    "        if (recall > best_scores[0]) or (recall == best_scores[0] and precision > best_scores[1]):\n",
    "            best_scores = (recall, precision, th)\n",
    "\n",
    "    best_threshold = best_scores[2]\n",
    "    \n",
    "    y_test = channel_data[\"label\"]\n",
    "    y_pred = predict(best_threshold, decision_scores)\n",
    "    print(\"best_threshold: \", best_threshold)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    print(\"FDR:\", 1 - precision)\n",
    "    c_m = confusion_matrix(y_test, y_pred)\n",
    "    print(c_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "207f134b-c3af-4e7b-9e60-df31cec0c957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel: T\n",
      "best_threshold:  -0.8992676803852276\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.68      0.75      1103\n",
      "           1       0.10      0.21      0.14       189\n",
      "\n",
      "    accuracy                           0.61      1292\n",
      "   macro avg       0.47      0.44      0.44      1292\n",
      "weighted avg       0.73      0.61      0.66      1292\n",
      "\n",
      "FDR: 0.2732372204520219\n",
      "[[754 349]\n",
      " [150  39]]\n",
      "channel: G\n",
      "best_threshold:  -3.894629500699023\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.71      0.82      2354\n",
      "           1       0.04      0.63      0.08        51\n",
      "\n",
      "    accuracy                           0.71      2405\n",
      "   macro avg       0.52      0.67      0.45      2405\n",
      "weighted avg       0.97      0.71      0.81      2405\n",
      "\n",
      "FDR: 0.03131591500501396\n",
      "[[1664  690]\n",
      " [  19   32]]\n",
      "channel: S\n",
      "best_threshold:  -1.1175199796552189\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.80      0.88       342\n",
      "           1       0.21      0.72      0.33        25\n",
      "\n",
      "    accuracy                           0.80       367\n",
      "   macro avg       0.59      0.76      0.60       367\n",
      "weighted avg       0.92      0.80      0.84       367\n",
      "\n",
      "FDR: 0.07682627807920661\n",
      "[[275  67]\n",
      " [  7  18]]\n",
      "channel: P\n",
      "best_threshold:  -9.51747161971224\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.82      0.86      1761\n",
      "           1       0.28      0.40      0.33       295\n",
      "\n",
      "    accuracy                           0.76      2056\n",
      "   macro avg       0.58      0.61      0.59      2056\n",
      "weighted avg       0.80      0.76      0.78      2056\n",
      "\n",
      "FDR: 0.19648492764676084\n",
      "[[1450  311]\n",
      " [ 176  119]]\n",
      "channel: D\n",
      "best_threshold:  -13.936624847258846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.71      0.75      3705\n",
      "           1       0.27      0.35      0.30      1084\n",
      "\n",
      "    accuracy                           0.63      4789\n",
      "   macro avg       0.53      0.53      0.53      4789\n",
      "weighted avg       0.67      0.63      0.65      4789\n",
      "\n",
      "FDR: 0.3285917762450171\n",
      "[[2649 1056]\n",
      " [ 703  381]]\n",
      "channel: F\n",
      "best_threshold:  -17.01574038184576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.68      0.75      1118\n",
      "           1       0.07      0.15      0.09       163\n",
      "\n",
      "    accuracy                           0.61      1281\n",
      "   macro avg       0.46      0.42      0.42      1281\n",
      "weighted avg       0.75      0.61      0.67      1281\n",
      "\n",
      "FDR: 0.2532303051001822\n",
      "[[759 359]\n",
      " [138  25]]\n",
      "channel: R\n",
      "best_threshold:  -49.600010294243354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.96       356\n",
      "           1       0.12      0.43      0.19         7\n",
      "\n",
      "    accuracy                           0.93       363\n",
      "   macro avg       0.55      0.68      0.58       363\n",
      "weighted avg       0.97      0.93      0.95       363\n",
      "\n",
      "FDR: 0.028575806477904475\n",
      "[[334  22]\n",
      " [  4   3]]\n",
      "channel: A\n",
      "best_threshold:  -6.958001415652674\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71      2818\n",
      "           1       0.04      0.08      0.05       556\n",
      "\n",
      "    accuracy                           0.56      3374\n",
      "   macro avg       0.41      0.37      0.38      3374\n",
      "weighted avg       0.66      0.56      0.61      3374\n",
      "\n",
      "FDR: 0.33918600989135517\n",
      "[[1849  969]\n",
      " [ 513   43]]\n",
      "channel: E\n",
      "best_threshold:  -9.027662819774122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.71      0.80      4882\n",
      "           1       0.15      0.40      0.22       604\n",
      "\n",
      "    accuracy                           0.68      5486\n",
      "   macro avg       0.53      0.56      0.51      5486\n",
      "weighted avg       0.82      0.68      0.73      5486\n",
      "\n",
      "FDR: 0.17750454974069996\n",
      "[[3479 1403]\n",
      " [ 361  243]]\n",
      "channel: B\n",
      "best_threshold:  -42.880753662252275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       397\n",
      "           1       1.00      0.33      0.50         6\n",
      "\n",
      "    accuracy                           0.99       403\n",
      "   macro avg       1.00      0.67      0.75       403\n",
      "weighted avg       0.99      0.99      0.99       403\n",
      "\n",
      "FDR: 0.009826550249685928\n",
      "[[397   0]\n",
      " [  4   2]]\n"
     ]
    }
   ],
   "source": [
    "for channel in standard_fitted_data:\n",
    "    one_class_svm(standard_fitted_data[channel], channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36fafb8e-9f0e-4cd6-b18d-d1b4902bda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolation_forest(channel_data, channel):\n",
    "    print(\"channel:\", channel)\n",
    "    clf = IsolationForest(random_state=42, n_jobs=-1).fit(channel_data[\"train\"])\n",
    "    decision_scores = clf.decision_function(channel_data[\"test\"])\n",
    "    \n",
    "    percentiles = np.arange(1, 30 + 1)\n",
    "    thresholds = []\n",
    "    for percent in percentiles: \n",
    "        thresholds.append(np.percentile(decision_scores, percent))\n",
    "    \n",
    "    # best_f1_score = 0\n",
    "    best_scores = (0, 0, 0)  \n",
    "\n",
    "    for th in thresholds:\n",
    "        y_pred = predict(th, decision_scores)\n",
    "        # f1 = f1_score(channel_data[\"label\"], y_pred)\n",
    "        recall = recall_score(channel_data[\"label\"], y_pred, pos_label=1)\n",
    "        precision = precision_score(channel_data[\"label\"], y_pred, zero_division=0)\n",
    "        if (recall > best_scores[0]) or (recall == best_scores[0] and precision > best_scores[1]):\n",
    "            best_scores = (recall, precision, th)\n",
    "\n",
    "    best_threshold = best_scores[2]\n",
    "    \n",
    "    y_test = channel_data[\"label\"]\n",
    "    y_pred = predict(best_threshold, decision_scores)\n",
    "    print(\"best_threshold: \", best_threshold)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    print(\"FDR:\", 1 - precision)\n",
    "    c_m = confusion_matrix(y_test, y_pred)\n",
    "    print(c_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7003185-2129-4e99-8fa6-bd5323a3b1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel: T\n",
      "best_threshold:  0.11642565349970714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.72      0.79      1103\n",
      "           1       0.19      0.40      0.26       189\n",
      "\n",
      "    accuracy                           0.67      1292\n",
      "   macro avg       0.53      0.56      0.52      1292\n",
      "weighted avg       0.77      0.67      0.71      1292\n",
      "\n",
      "FDR: 0.22566689441851684\n",
      "[[790 313]\n",
      " [114  75]]\n",
      "channel: G\n",
      "best_threshold:  0.03169341221337902\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.71      0.82      2354\n",
      "           1       0.02      0.22      0.03        51\n",
      "\n",
      "    accuracy                           0.70      2405\n",
      "   macro avg       0.50      0.46      0.43      2405\n",
      "weighted avg       0.96      0.70      0.80      2405\n",
      "\n",
      "FDR: 0.043807640922672086\n",
      "[[1667  687]\n",
      " [  40   11]]\n",
      "channel: S\n",
      "best_threshold:  0.05234593268931187\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.72      0.82       342\n",
      "           1       0.14      0.60      0.22        25\n",
      "\n",
      "    accuracy                           0.71       367\n",
      "   macro avg       0.55      0.66      0.52       367\n",
      "weighted avg       0.90      0.71      0.78       367\n",
      "\n",
      "FDR: 0.09509074138152052\n",
      "[[247  95]\n",
      " [ 10  15]]\n",
      "channel: P\n",
      "best_threshold:  0.0677566679369489\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.71      0.78      1761\n",
      "           1       0.17      0.35      0.23       295\n",
      "\n",
      "    accuracy                           0.66      2056\n",
      "   macro avg       0.52      0.53      0.50      2056\n",
      "weighted avg       0.77      0.66      0.70      2056\n",
      "\n",
      "FDR: 0.23298391493436454\n",
      "[[1248  513]\n",
      " [ 191  104]]\n",
      "channel: D\n",
      "best_threshold:  0.055670628203255954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.74      3705\n",
      "           1       0.24      0.32      0.28      1084\n",
      "\n",
      "    accuracy                           0.62      4789\n",
      "   macro avg       0.51      0.52      0.51      4789\n",
      "weighted avg       0.66      0.62      0.64      4789\n",
      "\n",
      "FDR: 0.34024134568392084\n",
      "[[2619 1086]\n",
      " [ 733  351]]\n",
      "channel: F\n",
      "best_threshold:  -0.03656736086037889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92      1118\n",
      "           1       0.15      0.02      0.04       163\n",
      "\n",
      "    accuracy                           0.86      1281\n",
      "   macro avg       0.51      0.50      0.48      1281\n",
      "weighted avg       0.78      0.86      0.81      1281\n",
      "\n",
      "FDR: 0.21824051897855046\n",
      "[[1096   22]\n",
      " [ 159    4]]\n",
      "channel: R\n",
      "best_threshold:  0.11140520349206817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.96       356\n",
      "           1       0.12      0.43      0.19         7\n",
      "\n",
      "    accuracy                           0.93       363\n",
      "   macro avg       0.55      0.68      0.58       363\n",
      "weighted avg       0.97      0.93      0.95       363\n",
      "\n",
      "FDR: 0.028575806477904475\n",
      "[[334  22]\n",
      " [  4   3]]\n",
      "channel: A\n",
      "best_threshold:  0.06693011946168796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.70      2818\n",
      "           1       0.02      0.03      0.02       556\n",
      "\n",
      "    accuracy                           0.55      3374\n",
      "   macro avg       0.40      0.34      0.36      3374\n",
      "weighted avg       0.65      0.55      0.59      3374\n",
      "\n",
      "FDR: 0.3520969749640279\n",
      "[[1824  994]\n",
      " [ 538   18]]\n",
      "channel: E\n",
      "best_threshold:  0.042384537292548874\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.69      0.77      4882\n",
      "           1       0.08      0.21      0.11       604\n",
      "\n",
      "    accuracy                           0.64      5486\n",
      "   macro avg       0.48      0.45      0.44      5486\n",
      "weighted avg       0.79      0.64      0.70      5486\n",
      "\n",
      "FDR: 0.211548788854379\n",
      "[[3365 1517]\n",
      " [ 475  129]]\n",
      "channel: B\n",
      "best_threshold:  0.08926204644377454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96       397\n",
      "           1       0.10      0.50      0.17         6\n",
      "\n",
      "    accuracy                           0.93       403\n",
      "   macro avg       0.55      0.72      0.57       403\n",
      "weighted avg       0.98      0.93      0.95       403\n",
      "\n",
      "FDR: 0.021250129834831766\n",
      "[[371  26]\n",
      " [  3   3]]\n"
     ]
    }
   ],
   "source": [
    "for channel in standard_fitted_data:\n",
    "    isolation_forest(standard_fitted_data[channel], channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baa41085-279e-46b1-b001-274dd234df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOF learns a reference density from X_train, new points are compared against training density\n",
    "def local_outlier_factor(channel_data, channel):\n",
    "    print(\"channel:\", channel)\n",
    "    clf = LocalOutlierFactor(n_neighbors=50, novelty=True)\n",
    "    clf.fit(channel_data[\"train\"])\n",
    "    decision_scores = clf.decision_function(channel_data[\"test\"])\n",
    " \n",
    "    percentiles = np.arange(1, 30 + 1)\n",
    "    thresholds = []\n",
    "    for percent in percentiles: \n",
    "        thresholds.append(np.percentile(decision_scores, percent))\n",
    "    \n",
    "    # best_f1_score = 0\n",
    "    best_scores = (0, 0, 0)  \n",
    "\n",
    "    for th in thresholds:\n",
    "        y_pred = predict(th, decision_scores)\n",
    "        # f1 = f1_score(channel_data[\"label\"], y_pred)\n",
    "        recall = recall_score(channel_data[\"label\"], y_pred, pos_label=1)\n",
    "        precision = precision_score(channel_data[\"label\"], y_pred, zero_division=0)\n",
    "        if (recall > best_scores[0]) or (recall == best_scores[0] and precision > best_scores[1]):\n",
    "            best_scores = (recall, precision, th)\n",
    "\n",
    "    best_threshold = best_scores[2]\n",
    "    \n",
    "    y_test = channel_data[\"label\"]\n",
    "    y_pred = predict(best_threshold, decision_scores)\n",
    "    print(\"best_threshold: \", best_threshold)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    print(\"FDR:\", 1 - precision)\n",
    "    c_m = confusion_matrix(y_test, y_pred)\n",
    "    print(c_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7c42bea-6020-47ad-adbb-85dcf6bb4ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel: T\n",
      "best_threshold:  0.48417609932572986\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.71      0.78      1103\n",
      "           1       0.15      0.29      0.20       189\n",
      "\n",
      "    accuracy                           0.65      1292\n",
      "   macro avg       0.50      0.50      0.49      1292\n",
      "weighted avg       0.75      0.65      0.69      1292\n",
      "\n",
      "FDR: 0.24958199270065595\n",
      "[[783 320]\n",
      " [134  55]]\n",
      "channel: G\n",
      "best_threshold:  -0.07318787799478454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.76      0.86      2354\n",
      "           1       0.05      0.61      0.10        51\n",
      "\n",
      "    accuracy                           0.75      2405\n",
      "   macro avg       0.52      0.68      0.48      2405\n",
      "weighted avg       0.97      0.75      0.84      2405\n",
      "\n",
      "FDR: 0.03096338683368427\n",
      "[[1784  570]\n",
      " [  20   31]]\n",
      "channel: S\n",
      "best_threshold:  0.37770101237544224\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.90       342\n",
      "           1       0.26      0.76      0.38        25\n",
      "\n",
      "    accuracy                           0.83       367\n",
      "   macro avg       0.62      0.80      0.64       367\n",
      "weighted avg       0.93      0.83      0.87       367\n",
      "\n",
      "FDR: 0.06971251784840427\n",
      "[[287  55]\n",
      " [  6  19]]\n",
      "channel: P\n",
      "best_threshold:  0.2558365719911555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88      1761\n",
      "           1       0.35      0.42      0.38       295\n",
      "\n",
      "    accuracy                           0.81      2056\n",
      "   macro avg       0.63      0.65      0.63      2056\n",
      "weighted avg       0.82      0.81      0.81      2056\n",
      "\n",
      "FDR: 0.17889086375344387\n",
      "[[1533  228]\n",
      " [ 171  124]]\n",
      "channel: D\n",
      "best_threshold:  0.32083333967042615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.74      3705\n",
      "           1       0.25      0.33      0.28      1084\n",
      "\n",
      "    accuracy                           0.62      4789\n",
      "   macro avg       0.52      0.52      0.51      4789\n",
      "weighted avg       0.66      0.62      0.64      4789\n",
      "\n",
      "FDR: 0.33791143179614\n",
      "[[2625 1080]\n",
      " [ 727  357]]\n",
      "channel: F\n",
      "best_threshold:  0.2031161049092347\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.72      0.80      1118\n",
      "           1       0.19      0.45      0.27       163\n",
      "\n",
      "    accuracy                           0.69      1281\n",
      "   macro avg       0.54      0.58      0.53      1281\n",
      "weighted avg       0.81      0.69      0.73      1281\n",
      "\n",
      "FDR: 0.1906221185611333\n",
      "[[807 311]\n",
      " [ 90  73]]\n",
      "channel: R\n",
      "best_threshold:  -1500282910145.0295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       356\n",
      "           1       0.27      0.43      0.33         7\n",
      "\n",
      "    accuracy                           0.97       363\n",
      "   macro avg       0.63      0.70      0.66       363\n",
      "weighted avg       0.97      0.97      0.97       363\n",
      "\n",
      "FDR: 0.025169045830202852\n",
      "[[348   8]\n",
      " [  4   3]]\n",
      "channel: A\n",
      "best_threshold:  0.3494826053783694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.72      2818\n",
      "           1       0.05      0.09      0.06       556\n",
      "\n",
      "    accuracy                           0.56      3374\n",
      "   macro avg       0.42      0.37      0.39      3374\n",
      "weighted avg       0.66      0.56      0.61      3374\n",
      "\n",
      "FDR: 0.3366038168768206\n",
      "[[1854  964]\n",
      " [ 508   48]]\n",
      "channel: E\n",
      "best_threshold:  0.24573731559026335\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.72      0.80      4882\n",
      "           1       0.16      0.43      0.23       604\n",
      "\n",
      "    accuracy                           0.68      5486\n",
      "   macro avg       0.53      0.57      0.51      5486\n",
      "weighted avg       0.83      0.68      0.74      5486\n",
      "\n",
      "FDR: 0.17332367827059914\n",
      "[[3493 1389]\n",
      " [ 347  257]]\n",
      "channel: B\n",
      "best_threshold:  -2845454463827.3633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       397\n",
      "           1       1.00      0.33      0.50         6\n",
      "\n",
      "    accuracy                           0.99       403\n",
      "   macro avg       1.00      0.67      0.75       403\n",
      "weighted avg       0.99      0.99      0.99       403\n",
      "\n",
      "FDR: 0.009826550249685928\n",
      "[[397   0]\n",
      " [  4   2]]\n"
     ]
    }
   ],
   "source": [
    "for channel in standard_fitted_data:\n",
    "    local_outlier_factor(standard_fitted_data[channel], channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02e59535-55f0-45e5-953a-d979443cfa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOF learns a reference density from X_train, new points are compared against training density\n",
    "def ensemble_local_outlier_factor(channel_data, channel):\n",
    "    clf = LocalOutlierFactor(n_neighbors=50, novelty=True)\n",
    "    clf.fit(channel_data[\"train\"])\n",
    "    decision_scores = clf.decision_function(channel_data[\"test\"])\n",
    " \n",
    "    percentiles = np.arange(1, 30 + 1)\n",
    "    thresholds = []\n",
    "    for percent in percentiles: \n",
    "        thresholds.append(np.percentile(decision_scores, percent))\n",
    "    \n",
    "    # best_f1_score = 0\n",
    "    best_scores = (0, 0, 0)  \n",
    "\n",
    "    for th in thresholds:\n",
    "        y_pred = predict(th, decision_scores)\n",
    "        # f1 = f1_score(channel_data[\"label\"], y_pred)\n",
    "        recall = recall_score(channel_data[\"label\"], y_pred, pos_label=1)\n",
    "        precision = precision_score(channel_data[\"label\"], y_pred, zero_division=0)\n",
    "        if (recall > best_scores[0]) or (recall == best_scores[0] and precision > best_scores[1]):\n",
    "            best_scores = (recall, precision, th)\n",
    "\n",
    "    best_threshold = best_scores[2]\n",
    "    \n",
    "    return predict(best_threshold, decision_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dd885e8-f592-4f7f-8291-2dca72912e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_isolation_forest(channel_data, channel):\n",
    "    clf = IsolationForest(random_state=42, n_jobs=-1).fit(channel_data[\"train\"])\n",
    "    decision_scores = clf.decision_function(channel_data[\"test\"])\n",
    "    \n",
    "    percentiles = np.arange(1, 30 + 1)\n",
    "    thresholds = []\n",
    "    for percent in percentiles: \n",
    "        thresholds.append(np.percentile(decision_scores, percent))\n",
    "    \n",
    "    # best_f1_score = 0\n",
    "    best_scores = (0, 0, 0)  \n",
    "\n",
    "    for th in thresholds:\n",
    "        y_pred = predict(th, decision_scores)\n",
    "        # f1 = f1_score(channel_data[\"label\"], y_pred)\n",
    "        recall = recall_score(channel_data[\"label\"], y_pred, pos_label=1)\n",
    "        precision = precision_score(channel_data[\"label\"], y_pred, zero_division=0)\n",
    "        if (recall > best_scores[0]) or (recall == best_scores[0] and precision > best_scores[1]):\n",
    "            best_scores = (recall, precision, th)\n",
    "\n",
    "    best_threshold = best_scores[2]\n",
    "    \n",
    "    return predict(best_threshold, decision_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57684881-eae2-41a5-86cc-df8c0b887dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_one_class_svm(channel_data, channel):\n",
    "    clf = OneClassSVM().fit(channel_data[\"train\"])\n",
    "    decision_scores = clf.decision_function(channel_data[\"test\"])\n",
    "   \n",
    "    percentiles = np.arange(1, 30 + 1)\n",
    "    thresholds = []\n",
    "    for percent in percentiles: \n",
    "        thresholds.append(np.percentile(decision_scores, percent))\n",
    "    \n",
    "    # best_f1_score = 0\n",
    "    best_scores = (0, 0, 0)  \n",
    "\n",
    "    for th in thresholds:\n",
    "        y_pred = predict(th, decision_scores)\n",
    "        # f1 = f1_score(channel_data[\"label\"], y_pred)\n",
    "        recall = recall_score(channel_data[\"label\"], y_pred, pos_label=1)\n",
    "        precision = precision_score(channel_data[\"label\"], y_pred, zero_division=0)\n",
    "        if (recall > best_scores[0]) or (recall == best_scores[0] and precision > best_scores[1]):\n",
    "            best_scores = (recall, precision, th)\n",
    "\n",
    "    best_threshold = best_scores[2]\n",
    "    \n",
    "    return predict(best_threshold, decision_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48f7d7e1-4479-493d-9695-b446072ab73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel:  T\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.35      0.50      1103\n",
      "           1       0.15      0.67      0.25       189\n",
      "\n",
      "    accuracy                           0.40      1292\n",
      "   macro avg       0.51      0.51      0.37      1292\n",
      "weighted avg       0.76      0.40      0.46      1292\n",
      "\n",
      "FDR: 0.021250129834831766\n",
      "[[385 718]\n",
      " [ 62 127]]\n",
      "channel:  G\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.52      0.69      2354\n",
      "           1       0.04      0.80      0.07        51\n",
      "\n",
      "    accuracy                           0.53      2405\n",
      "   macro avg       0.51      0.66      0.38      2405\n",
      "weighted avg       0.97      0.53      0.67      2405\n",
      "\n",
      "FDR: 0.021250129834831766\n",
      "[[1233 1121]\n",
      " [  10   41]]\n",
      "channel:  S\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.62      0.76       342\n",
      "           1       0.13      0.80      0.23        25\n",
      "\n",
      "    accuracy                           0.63       367\n",
      "   macro avg       0.55      0.71      0.49       367\n",
      "weighted avg       0.92      0.63      0.72       367\n",
      "\n",
      "FDR: 0.021250129834831766\n",
      "[[211 131]\n",
      " [  5  20]]\n",
      "channel:  P\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76      1761\n",
      "           1       0.20      0.51      0.29       295\n",
      "\n",
      "    accuracy                           0.65      2056\n",
      "   macro avg       0.55      0.59      0.53      2056\n",
      "weighted avg       0.79      0.65      0.70      2056\n",
      "\n",
      "FDR: 0.021250129834831766\n",
      "[[1179  582]\n",
      " [ 145  150]]\n",
      "channel:  D\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.54      0.65      3705\n",
      "           1       0.28      0.62      0.39      1084\n",
      "\n",
      "    accuracy                           0.56      4789\n",
      "   macro avg       0.55      0.58      0.52      4789\n",
      "weighted avg       0.70      0.56      0.59      4789\n",
      "\n",
      "FDR: 0.021250129834831766\n",
      "[[1995 1710]\n",
      " [ 415  669]]\n",
      "channel:  F\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.65      0.75      1118\n",
      "           1       0.16      0.45      0.23       163\n",
      "\n",
      "    accuracy                           0.63      1281\n",
      "   macro avg       0.52      0.55      0.49      1281\n",
      "weighted avg       0.80      0.63      0.69      1281\n",
      "\n",
      "FDR: 0.021250129834831766\n",
      "[[730 388]\n",
      " [ 90  73]]\n",
      "channel:  R\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.96       356\n",
      "           1       0.12      0.43      0.19         7\n",
      "\n",
      "    accuracy                           0.93       363\n",
      "   macro avg       0.55      0.68      0.58       363\n",
      "weighted avg       0.97      0.93      0.95       363\n",
      "\n",
      "FDR: 0.021250129834831766\n",
      "[[334  22]\n",
      " [  4   3]]\n",
      "channel:  A\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.50      0.60      2818\n",
      "           1       0.05      0.12      0.07       556\n",
      "\n",
      "    accuracy                           0.44      3374\n",
      "   macro avg       0.39      0.31      0.33      3374\n",
      "weighted avg       0.63      0.44      0.51      3374\n",
      "\n",
      "FDR: 0.021250129834831766\n",
      "[[1401 1417]\n",
      " [ 489   67]]\n",
      "channel:  E\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.54      0.67      4882\n",
      "           1       0.13      0.55      0.21       604\n",
      "\n",
      "    accuracy                           0.54      5486\n",
      "   macro avg       0.52      0.54      0.44      5486\n",
      "weighted avg       0.82      0.54      0.62      5486\n",
      "\n",
      "FDR: 0.021250129834831766\n",
      "[[2618 2264]\n",
      " [ 273  331]]\n",
      "channel:  B\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96       397\n",
      "           1       0.10      0.50      0.17         6\n",
      "\n",
      "    accuracy                           0.93       403\n",
      "   macro avg       0.55      0.72      0.57       403\n",
      "weighted avg       0.98      0.93      0.95       403\n",
      "\n",
      "FDR: 0.021250129834831766\n",
      "[[371  26]\n",
      " [  3   3]]\n"
     ]
    }
   ],
   "source": [
    "for channel in standard_fitted_data: \n",
    "    y_pred_one_class_svm = ensemble_one_class_svm(standard_fitted_data[channel], channel)\n",
    "    y_pred_isolation_forest = ensemble_isolation_forest(standard_fitted_data[channel], channel)\n",
    "    y_pred_local_outlier_factor = ensemble_local_outlier_factor(standard_fitted_data[channel], channel)\n",
    "\n",
    "    y_pred = []\n",
    "    for i in range(len(y_pred_one_class_svm)): \n",
    "        ensemble = y_pred_one_class_svm[i] + y_pred_isolation_forest[i] + y_pred_local_outlier_factor[i]\n",
    "        if ensemble >= 1: \n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    print(\"channel: \",channel)\n",
    "    y_test = (standard_fitted_data[channel][\"label\"])\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    precision_1 = precision_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "    print(\"FDR:\", 1 - precision)\n",
    "    c_m = confusion_matrix(y_test, y_pred)\n",
    "    print(c_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78118a32-64eb-4025-817c-751296dc118e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d894700-52ed-4938-9791-ef3b9af3e19c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python-3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
