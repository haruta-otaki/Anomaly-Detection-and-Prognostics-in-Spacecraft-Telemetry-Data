% Describe any background \cite{aima} nformation that the reader would need to know
% to understand your work. You do not have to explain algorithms or
% ideas that we have seen in class. Rather, use this section to describe
% techniques that you found elsewhere in the course of your research,
% that you have decided to bring to bear on the problem at hand. Don't
% go overboard here --- if what you're doing is quite detailed, it's
% often more helpful to give a sketch of the big ideas of the approaches
% that you will be using. You can then say something like ``the reader
% is referred to X for a more in-depth description of...'', and include
% a citation.\\

% This section is also a good place to describe any data pre-processing
% or feature engineering you may have performed. If you are \emph{only}
% discussing data wrangling in this section, it's recommended that you
% amend the title of the section to ``Data Preparation'' or something
% similar; otherwise, use subsections to better organize the flow.

\section{Background}
\label{sec:background}

This section describes the dataset used for training and testing the model, as well as the feature engineering and classification techniques employed in our unsupervised machine learning approach to anomaly detection. We first outline the data preprocessing pipeline, followed by an exploration of the unsupervised learning algorithms integrated to produce binary anomaly classifications.

\subsection{Data Exploration}
The dataset consists of real-world, multivariate telemetry anomaly data collected from the Soil Moisture Active Passive (SMAP) satellite and the Curiosity Rover on Mars (MSL), with telemetry anomalies previously reviewed and documented in ISA reports. For each telemetry channel, specific anomalous time intervals were manually labelled and categorised into two types: point anomalies, which are typically detectable by well-calibrated alarms, and contextual anomalies, which require more complex temporal analysis to identify \cite{hundman2018smap}.

In this work, we exclusively utilize telemetry data from the SMAP satellite, as the SMAP and MSL datasets differ in sensor dimensionality, respectively containing 25 and 55 features. The dataset was provided with predefined training and testing splits, where each telemetry channel is stored as a separate \texttt{.npy} file identified by a unique channel ID. Each file is structured with timesteps as rows and multivariate sensor readings as columns.

Channel identifiers and names are encoded in the filenames. To promote generalisability, all telemetry streams were anonymised using a standardised "letter–number" naming convention, where the leading letter denotes the general category of the channel (e.g., \textbf{P} for Power, \textbf{R} for Radiation, \textbf{T} for Temperature). Additionally, a \texttt{labelled\_anomalies.csv} file is provided, containing metadata for each channel, including the start and end indices of ground-truth anomaly intervals, the anomaly class (point or contextual), and the originating spacecraft \cite{hundman2018smap}.

\subsection{Feature Selection}
In the pre-processing of the raw dataset, this paper adopts an overlapping window approach in which contiguous groups of timestamps are treated as individual samples, providing a context-aware and robust representation of anomalous behaviour. Each window spans 60 timestamps; since each timestamp corresponds to one minute of aggregated telemetry, this window length captures hour-scale temporal patterns. Consecutive windows overlap by 40 timestamps, a design choice that mitigates edge effects where anomalies may occur near window boundaries and introduces a smoothing effect by evaluating the same timestamps multiple times. As a result, this approach reduces sensitivity to short-lived fluctuations while ensuring that each timestamp is assessed within the context of its surrounding temporal neighbourhood rather than in isolation; this encourages the learning algorithms to capture temporal trends and patterns, whilst promoting high recall: a single anomalous timestamp contributes to the anomaly scores of multiple windows, increasing its likelihood of detection. \\
Separate models are trained for each type of telemetry channel to account for differences in the underlying physical characteristics of the sensor data. Channel types are identified by the leading letter in the dataset's filenames. For channels consisting of multiple datasets distinguished by unique IDs, the data were aggregated with care to ensure that sliding windows did not span across different IDs, as these sequences do not represent continuous time series. Ground-truth labels for the test data were derived using the anomaly metadata provided with the dataset: a window was labelled as anomalous if it contained at least one anomalous timestamp within its 60-minute span. No labels were used during training, as the models operate in an unsupervised setting. Metadata containing the dataset statistics after feature engineering is included in Table \ref{tab:dataset-summary}. 

\begin{table}[ht]
\centering
\caption{Dataset statistics, post feature engineering}
\label{tab:dataset-summary}
\begin{tabular}{lc}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Total anomaly sequences & 2980 \\
Point anomalies & 62\% \\
Contextual anomalies & 38\% \\
Unique telemetry channels & 10 \\
Telemetry values evaluated & 21{,}816 \\
\hline
\end{tabular}
\end{table}

\subsection{Feature Engineering}
We employ two feature engineering techniques to preprocess the raw dataset and enhance the performance of the unsupervised learning models evaluated in this study: MiniRocket and z-score scaling.

\subsection{MiniRocket}
MiniRocket is a time series classification method that leverages simple linear classifiers combined with random convolutional kernels to achieve state-of-the-art accuracy at a fraction of the computational cost of existing approaches \cite{dempster2020minirocket}. In this work, we employ \texttt{MiniRocketMultivariate}, an extension of MiniRocket designed for multivariate time series, making it well suited to the telemetry streams used in this study. Specifically, we utilize the implementation provided by the sktime library, which applies convolutional kernels of fixed length 9 with weights constrained to two possible values. The method initializes 84 fixed base convolutions—each composed of six instances of one weight and three of the other—which are then used to generate dilated convolutions for feature extraction.
\subsection{Z-Score Scaling}

Feature scaling was applied during the preprocessing of the data for the predictive models in this paper to ensure that all features were on comparable scales. The \texttt{StandardScaler} from the scikit-learn library was used to implement Z-score scaling, transforming each input to have a mean of zero and a standard deviation of one. Each feature was centred and scaled independently, resulting in a standardised distribution around zero.

\subsection{Models}
We employed three unsupervised learning models to perform anomaly detection on the provided multivariate telemetry data: One-Class SVM, Isolation Forest, and Local Outlier Factor (LOF).

\subsection{One-Class SVM}
One-Class Support Vector Machine (One-Class SVM) is a variant of the standard Support Vector Machine designed specifically for outlier, anomaly, or novelty detection. Its objective is to identify instances that deviate significantly from the norm. Unlike traditional SVM, One-Class SVM is trained solely on the majority (normal) class and defines a boundary that encloses normal data points, enabling the detection of outliers or novel instances in the testing dataset. In this study, we implemented One-Class SVM using the \texttt{OneClassSVM} class from the scikit-learn library.

\subsection{Isolation Forest}
Isolation Forest is an unsupervised learning model that recursively partitions the training data by randomly selecting a feature and a split value for each node in its tree structure. The algorithm measures the path length from the root node to each leaf, assigning higher anomaly scores to samples that reach a leaf in relatively few steps. The average path length across the forest of random trees serves as a metric of normality. In this study, we implemented Isolation Forest using the \texttt{IsolationForest} class from the scikit-learn library.

\subsection{Local Outlier Factor}
The Local Outlier Factor (LOF) is an unsupervised outlier detection method that assigns an anomaly score to each sample by measuring the local deviation of its density relative to its neighbours. A sample is considered an outlier if its density is significantly lower than that of its surrounding points, effectively quantifying how isolated it is within its neighbourhood based on a chosen distance metric. In this study, we implemented LOF using the \texttt{LocalOutlierFactor} class from the scikit-learn library.